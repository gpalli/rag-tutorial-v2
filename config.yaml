# RAG System Configuration
# This file contains default configurations for different embedding models and providers

# Default embedding configuration
embedding:
  provider: "ollama"  # ollama, openai, huggingface, bedrock, sentence_transformers
  model: null  # null means use default for provider
  ollama:
    base_url: "http://localhost:11434"
    timeout: 120
  openai:
    api_key: null  # Set via environment variable OPENAI_API_KEY
  huggingface:
    api_key: null  # Set via environment variable HUGGINGFACE_API_KEY
    cache_dir: "./cache"
  bedrock:
    aws_profile: "default"
    aws_region: "us-east-1"

# Default LLM configuration
llm:
  provider: "ollama"
  model: "mistral"
  ollama:
    base_url: "http://localhost:11434"

# Database configuration
database:
  chroma_path: "chroma"
  data_path: "data"

# Document processing configuration
documents:
  chunk_size: 800
  chunk_overlap: 80
  supported_formats: ["pdf", "md", "txt", "rtf"]

# Query configuration
query:
  max_results: 5
  similarity_threshold: 0.0
